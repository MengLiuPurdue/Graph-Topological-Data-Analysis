{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This tutorial walks through the steps of training a ResNet50 model for Fashion MNIST dataset and analyzing the results with GTDA.\n",
    "Please first setup your environment using the included GTDA.yml file in this repo or manually.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision.models as torch_models\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning import Trainer,seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ResNet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We directly used the ImageNet pretrained ResNet50 model from pytorch as a backbone to train our own model. \n",
    "Since Fashion MNIST dataset only has 10 different classes, we need to replace the last fully connected layer.\n",
    "We also need to add definitions of \"training_step\", \"validation_step\", \"test_step\" and \"configure_optimizers\" in order to use pytorch lightning framework.\n",
    "\"\"\"\n",
    "class MyResNet(pl.LightningModule):\n",
    "    def __init__(self, args, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.model = torch_models.resnet18(pretrained=True)\n",
    "        # Change the input layer to take Grayscale image, instead of RGB images. \n",
    "        # Hence in_channels is set as 1\n",
    "        # original definition of the first layer on the ResNet class\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        num_filters = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_filters, args.num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        images, labels = batch\n",
    "        predictions = self.model(images)\n",
    "        loss = self.criterion(predictions, labels)\n",
    "        accuracy = self.accuracy(predictions, labels)\n",
    "        return loss, accuracy * 100\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        loss, accuracy = self.forward(batch)\n",
    "        self.log(\"loss/train\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"acc/train\", accuracy, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss, accuracy = self.forward(batch)\n",
    "        self.log(\"loss/val\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"acc/val\", accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        loss, accuracy = self.forward(batch)\n",
    "        self.log(\"acc/test\", accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = self.model.parameters()\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.args.learning_rate)\n",
    "        return optimizer\n",
    "    # def configure_optimizers(self):\n",
    "    #     if self.args.last_layer_only:\n",
    "    #         parameters = self.classifier.parameters()\n",
    "    #     else:\n",
    "    #         parameters = self.model.parameters()\n",
    "    #     optimizer = torch.optim.SGD(\n",
    "    #         parameters,\n",
    "    #         lr=self.args.learning_rate,\n",
    "    #         weight_decay=self.args.weight_decay,\n",
    "    #         momentum=0.9,\n",
    "    #         nesterov=True,\n",
    "    #     )\n",
    "    #     scheduler = {\n",
    "    #         \"scheduler\": StepLR(\n",
    "    #             optimizer, step_size=self.args.lr_step_size, gamma=self.args.lr_gamma,\n",
    "    #         ),\n",
    "    #         \"interval\": \"epoch\",\n",
    "    #         \"name\": \"learning_rate\",\n",
    "    #     }\n",
    "    #     return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFashionMNIST(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, args, train_transform=None, test_transform=None):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.train_dataset = FashionMNIST(root=\"../dataset/\",train=True,download=True,transform=train_transform)\n",
    "        self.test_dataset = FashionMNIST(root=\"../dataset/\",train=False,download=True,transform=test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            num_workers=self.args.num_workers,\n",
    "            shuffle=self.args.shuffle,\n",
    "            drop_last=self.args.drop_last,\n",
    "            pin_memory=self.args.pin_memory,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            num_workers=self.args.num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=self.args.pin_memory,\n",
    "            shuffle=False\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/liu1740/miniconda3/envs/GTDA/lib/python3.9/site-packages/torchvision/datasets/mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "Global seed set to 42\n",
      "/homes/liu1740/miniconda3/envs/GTDA/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"batch_size\": 256,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_epochs\": 100,\n",
    "    \"num_workers\": 8,\n",
    "    \"num_classes\": 10,\n",
    "    \"precision\": 32,\n",
    "    \"gpu_id\": 0,\n",
    "    \"shuffle\": True,\n",
    "    \"drop_last\": False,\n",
    "    \"pin_memory\": True,\n",
    "    \"lr_warmup\": 0.2,\n",
    "    \"lr_gamma\": 0.1,\n",
    "    \"lr_step_size\": 20,\n",
    "}\n",
    "args = Namespace(**args)\n",
    "\n",
    "\"\"\"\n",
    "For training transform, we use standard normalization and data augmentation.\n",
    "\"\"\"\n",
    "fashion_mnist = FashionMNIST(root=\"../dataset/\",train=True,download=True).train_data.float()\n",
    "\n",
    "train_transform = transforms.Compose([ transforms.Resize((112, 112)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomAffine(degrees=20, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n",
    "\n",
    "test_transform = transforms.Compose([ transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n",
    "\n",
    "\n",
    "# train_transform = transforms.Compose([ transforms.Resize((128, 128)),\n",
    "#         transforms.RandomSizedCrop(112),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(), \n",
    "#         transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n",
    "\n",
    "# test_transform = transforms.Compose([ transforms.Resize((112, 112)),\n",
    "#         transforms.ToTensor(), \n",
    "#         transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,))])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=\"acc/val\", mode=\"max\", save_last=True)\n",
    "seed_everything(42, workers=True)\n",
    "lightning_model = MyResNet(args)\n",
    "logger = TensorBoardLogger(\"FashionMNIST\", name=\"resnet50\")\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    gpus=-1,\n",
    "    deterministic=True,\n",
    "    weights_summary=None,\n",
    "    log_every_n_steps=1,\n",
    "    max_epochs=args.max_epochs,\n",
    "    callbacks=[checkpoint],\n",
    "    precision=args.precision,\n",
    ")\n",
    "data = MyFashionMNIST(\n",
    "    args,train_transform=train_transform,\n",
    "    test_transform=test_transform)\n",
    "assert(data.train_dataset.class_to_idx==data.test_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.fit(lightning_model,data)\n",
    "ckpt = torch.load(\"FashionMNIST/resnet50/version_14/checkpoints/last.ckpt\")\n",
    "lightning_model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de5994b1f0b4879bb875b339e94f1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'acc/test': 93.9800033569336}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'acc/test': 93.9800033569336}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=lightning_model,dataloaders=data.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from GTDA.GTDA_utils import SPoC,normalize,knn_cuda_graph\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.sparse as sp\n",
    "from GTDA.GTDA_utils import compute_reeb, NN_model\n",
    "from GTDA.GTDA import GTDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:03<00:00, 73.21it/s] \n",
      "100%|██████████| 40/40 [00:01<00:00, 29.27it/s]\n",
      "100%|██████████| 235/235 [00:03<00:00, 72.37it/s] \n",
      "100%|██████████| 40/40 [00:01<00:00, 29.23it/s]\n",
      "100%|██████████| 274/274 [00:17<00:00, 15.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cnn_model = lightning_model.model\n",
    "cnn_model.eval()\n",
    "args.shuffle = False\n",
    "args.drop_last = False\n",
    "data_orig = MyFashionMNIST(\n",
    "    args,train_transform=test_transform,\n",
    "    test_transform=test_transform)\n",
    "trainset_orig = data_orig.train_dataset\n",
    "testset_orig = data_orig.test_dataset\n",
    "trainloader_orig = data_orig.train_dataloader()\n",
    "testloader_orig = data_orig.test_dataloader()\n",
    "train_nodes = list(range(len(trainset_orig)))\n",
    "val_nodes = []\n",
    "test_nodes = list(range(len(trainset_orig),len(trainset_orig)+len(testset_orig)))\n",
    "_,y,preds_orig = SPoC(cnn_model,[trainloader_orig,testloader_orig],pooling='avg')\n",
    "y = np.array(y)\n",
    "X_orig,_,_ = SPoC(cnn_model,[trainloader_orig,testloader_orig],pooling='max')\n",
    "\n",
    "pca = PCA(n_components=128,random_state=42)\n",
    "Xr_orig = pca.fit_transform(X_orig)\n",
    "Dinv = sp.spdiags(1/pca.singular_values_,0,Xr_orig.shape[1],Xr_orig.shape[1])\n",
    "Xr_orig = Xr_orig@Dinv\n",
    "Xr_orig = normalize(Xr_orig)\n",
    "Xr_orig = torch.tensor(Xr_orig).to('cuda')\n",
    "A_knn_orig = knn_cuda_graph(Xr_orig,5,256)\n",
    "G = (A_knn_orig>0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess lens..\n",
      "Merge reeb nodes...\n",
      "Build reeb graph...\n",
      "Total time for building reeb graph is 13.817661762237549 seconds\n",
      "Compute mixing rate for each sample\n"
     ]
    }
   ],
   "source": [
    "nn_model = NN_model()\n",
    "nn_model.preds = preds_orig\n",
    "nn_model.labels = y\n",
    "nn_model.A = G\n",
    "nn_model.train_mask = np.zeros(G.shape[0])\n",
    "nn_model.train_mask[train_nodes] = 1\n",
    "nn_model.val_mask = np.zeros(G.shape[0])\n",
    "nn_model.val_mask[val_nodes] = 1\n",
    "nn_model.test_mask = np.zeros(G.shape[0])\n",
    "nn_model.test_mask[test_nodes] = 1\n",
    "smallest_component = 100\n",
    "overlap = 0.025\n",
    "labels_to_eval = list(range(preds_orig.shape[1]))\n",
    "GTDA_record = compute_reeb(GTDA,nn_model,labels_to_eval,smallest_component,overlap,extra_lens=None,\n",
    "    node_size_thd=5,reeb_component_thd=5,nprocs=10,device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GTDA.GTDA_utils import save_to_json\n",
    "label_to_name = {\n",
    "    0:\"T-shirt/top\",\n",
    "    1:\"Trouser\",\n",
    "    2:\"Pullover\",\n",
    "    3:\"Dress\",\n",
    "    4:\"Coat\",\n",
    "    5:\"Sandal\",\n",
    "    6:\"Shirt\",\n",
    "    7:\"Sneaker\",\n",
    "    8:\"Bag\",\n",
    "    9:\"Ankle boot\",\n",
    "}\n",
    "save_to_json(GTDA_record, nn_model, \".\", label_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('GTDA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28aac9773727333c249ca4fdeb162f21bd36b4a6cc271ed2a790623cd29eb89d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
